{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4079ebb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CMP-3004\n",
    "## Computer Organization\n",
    "\n",
    "### Spring 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c5de9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flynn’s taxonomy\n",
    "\n",
    "![](./multi1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead5498",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Flynn’s Taxonomy\n",
    "\n",
    "- **SISD:** A single processor executes a single instruction stream (Uniprocessors)\n",
    "\n",
    "- **SIMD:** Multiprocessors must execute the same instruction simultaneously on multiple data\n",
    "\n",
    "- **MISD:** Each processor executes a different instruction sequence on a unique data set\n",
    "\n",
    "- **MIMD:** A set of processors simultaneously execute different instruction sequences on different data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d74e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computer architecture\n",
    "\n",
    "![](./multi2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(a,b,c):\n",
    "    \n",
    "    return a + b + c\n",
    "\n",
    "add x1, x2, x3\n",
    "add x3, x4, x5\n",
    "ret x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b266fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50166832",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel and multiprocessor\n",
    "\n",
    "- Paralleslism results in \n",
    "    - higher throughput\n",
    "    - better fault tolerance\n",
    "    - better cost-performance ratio\n",
    "- If we have $n$ processors running in parallel, a computation job should take $1/n$ of the time (perfect speedup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba47bcc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Amdahl's law\n",
    "\n",
    "Perfect speedup is not possible\n",
    "- There is always part of the work done by a single processor that needs to be done serially\n",
    "- The greater the sequential processing, the less cost effective is the architecture\n",
    "\n",
    "![](./raid6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30006668",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symmetric multiprocessors\n",
    "\n",
    "- Two or more similar processors of comparable capability\n",
    "- A bus or other connection scheme is used to share the main memory and I/O facilities\n",
    "- All processors share access to I/O devices\n",
    "- All processors can perform the same functions\n",
    "- Integrated operating system hat provides interaction between processors and their programs\n",
    "- Processes or threads are scheduled across all of the processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbf61a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symmetric multiprocessors\n",
    "\n",
    "![](./multi3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31d49b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Communication between processors\n",
    "\n",
    "- Messages and status information left in common data areas\n",
    "- Each processor may also have its own private main memory and I/O channels in addition to the shared resources\n",
    "\n",
    "\n",
    "![](./multi4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d1447",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symmetric multiprocessor organization\n",
    "\n",
    "The time-shared bus is the simplest mechanism for constructing a multiprocessor system\n",
    "\n",
    "- Simplicity: the same as the single-processor system\n",
    "- Flexibility: easy to expand\n",
    "- Reliability: a failure of an attached device does not propagate (passive medium)\n",
    "\n",
    "![](./multi5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61822d40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symmetric multiprocessor organization\n",
    "\n",
    "- The main drawback to the bus organization is performance\n",
    "    - the bus cycle time limits the speed of the system\n",
    "- Cache memory per processor\n",
    "    - reduce the number of bus accesses dramatically\n",
    "- Two or three levels of cache: \n",
    "    - L1 cache internal\n",
    "    - L2 cache either internal or external\n",
    "    - Some processors now employ a L3 cache as well\n",
    "- Cache coherence problem\n",
    "    - if a word is altered in one cache, it could conceivably invalidate a word in another cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024347c7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symmetric multiprocessor organization\n",
    "\n",
    "**MESI protocol:** to provide cache consistency on an SMP. The data cache includes two status bits per tag\n",
    "\n",
    "- Modified: The line has been modified (different from main memory) and is available only in this cache\n",
    "- Exclusive: The line is the same as that in main memory and is not present in any other cache\n",
    "- Shared: The line is the same as that in main memory and may be present in another cache\n",
    "- Invalid: The line does not contain valid data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba31b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MESI cache line states\n",
    "\n",
    "![](./multi6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d535294",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MESI State Transition Diagram\n",
    "\n",
    "![](./multi7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e5ee9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### L1-L2 cache consistency\n",
    "\n",
    "- L1 cache that does not connect directly to the bus\n",
    "- Some scheme is needed to maintain data integrity across both levels of cache and across all caches in the SMP configuration\n",
    "- Extend the MESI protocol to the L1 caches (L1 cache includes bits to indicate the state)\n",
    "- **Goal:** for any line that is present in both an L2 cache and its corresponding L1 cache, the L1 line state should track the state of the L2 line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3880de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compute Unified Device Architecture (CUDA)\n",
    "\n",
    "- Software architecture that enables GPUs to be programmed using high-level programming languages such as C and C++\n",
    "- CUDA requires an NVIDIA GPU\n",
    "- CUDA is an elegant solution to the problem of representing parallelism in algorithms\n",
    "    - not all algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1922a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic definitions\n",
    "\n",
    "- A CUDA program can be divided into\n",
    "    - Code to be run on the host (CPU)\n",
    "    - Code to be run on the device (GPU)\n",
    "    - The code related to the transfer of data between the host and the device\n",
    "\n",
    "- The **host** (CPU) interfaces with the user and controls the **device** (GPU) \n",
    "    - The host executes the serial portion of the application\n",
    "\n",
    "- The **device** (GPU) is connected to the host\n",
    "    - The device executes the data-parallel, compute-intensive portion of an application\n",
    "\n",
    "- **Kernel** is the code executed by the **device** \n",
    "    - It is a function callable from the host and executed in parallel by many threads\n",
    "    - An application or library function might consist of one or more kernels\n",
    "    - A kernel can be written in C language with additional key words to express parallelism\n",
    "\n",
    "- A **thread** is a single instance of the kernel function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fe067",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GPU v. CPU\n",
    "\n",
    "![](./gpu1.png)\n",
    "\n",
    "- GPU consists of streaming multiprocessors (SM)\n",
    "- GPU uses a massively parallel SIMD (single instruction multiple data) architecture to perform mainly mathematical operations\n",
    "- A GPU doesn’t require the same complex capabilities of the CPU’s control logic \n",
    "    - out of order execution, branch prediction, data hazards\n",
    "    - doesn't require large amounts of cache memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a406a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NVIDIA Fermi architecture\n",
    "\n",
    "![](./gpu2.png)\n",
    "\n",
    "- 16 streaming multiprocessor (SM)\n",
    "    - 32 cuda cores each\n",
    "- Every cuda is an execute unit for integer and float numbers\n",
    "- GigaThread is the global scheduler\n",
    "    - responsible for the distribution of thread blocks to all of the SM’s warp schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039bb20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Streaming multiprocessor\n",
    "\n",
    "![](./gpu3.png)\n",
    "\n",
    "- 16 streaming multiprocessor (SM)\n",
    "    - 32 cuda cores each\n",
    "- Every cuda is an execute unit for integer and float numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839376a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Warp scheduler\n",
    "\n",
    "![](./gpu6.png)\n",
    "\n",
    "- The dual warp scheduler will break up each thread block it is processing into warps\n",
    "- A warp is a bundle of 32 threads that start at the same starting address and their thread IDs are consecutive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e75b54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Special function units (SFUs)\n",
    "\n",
    "- Each SM has four SFUs\n",
    "- The SFU performs transcendental operations, such as cosine, sine, reciprocal, and square root, in a single clock cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc80f27",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load/store units (LD/ST)\n",
    "\n",
    "Fetch and save data to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56895ce9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Memory hierarchy\n",
    "\n",
    "![](./gpu4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a0dd0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Memory hierarchy\n",
    "\n",
    "![](./gpu5.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
